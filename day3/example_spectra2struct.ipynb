{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557ad52-0a80-406d-ab3f-3588911d47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ee7117-1dbb-450f-8620-8c116bb21d36",
   "metadata": {},
   "source": [
    "# Molecular structure elucidation from NMR spectra: Part 2\n",
    "\n",
    "Now that we have built and pretrained our substructure to structure model, let us now in the second part of this exercise integrate this into the full multi-task model that we can use for spectra to structure predictions. Figure 3 below summarizes the architectual details of the multi-task model (highlighted with the green box) that we will build and train for predicting structure from NMR spectra. Note that we will slot in our pretrained substructure to structure model from the previous part (highlighted with the black box) directly into the \"Encoder-Decoder Transformer\" of the part of the spectra to structure model.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"figures/architecture2.png\" width=\"80%\"/>\n",
    "    <figcaption><i>Figure 3: Diagram summarizing the architecture of our multi-task model used for spectra to substructure and spectra to structure predictions</i></figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea315b-cc67-4b85-b5f0-2a338f851a99",
   "metadata": {},
   "source": [
    "## Training our multi-task model\n",
    "\n",
    "Just like in part 1, since the training of our multi-task model will take some time, let us first get that started before we dive into the other details. Training our model will be straightforward since we will use the full implementation of the model provided by the NMR2Struct software package. To start training the model:\n",
    "\n",
    "1. Login to the NYU Shanghai cluster and navigate to where you would like to perform the training of the model.\n",
    "\n",
    "```\n",
    "ssh <user>@hpclogin.shanghai.nyu.edu\n",
    "```\n",
    "\n",
    "2. Navigate to the run directory for exercise 2 then submit the training run to the job scheduler:\n",
    "\n",
    "```\n",
    "cd nmr2mol/exercise_part2/\n",
    "sbatch -J multitask-train submit.sh\n",
    "```\n",
    "\n",
    "3. You can monitor the progress training your model by downloading the tfevents file generated to your local machine and using Tensorboard as show below and navigating in your web browser to the indicated URL (e.g., \"http://localhost:6006/\"). Note that the losses used to optimize our model parameters and that are visualized via Tensorboard are the standard cross-entropy losses\n",
    "\n",
    "```\n",
    "rsync -avz \"<user>@hpclogin.shanghai.nyu.edu:~/nmr2mol/exercise_part2/checkpoints/events.out.*\" model2/\n",
    "conda activate NMR_env\n",
    "tensorboard --logdir=model2/\n",
    "\n",
    "```\n",
    "\n",
    "Now you just need to wait for your job to run and your model will then start training, which will take on the order of an hour. In the meantime, let us understand the details of the model we are training by building our own version of it from scratch using Pytorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162d4de-dd02-46a3-a69a-4e5ef9eb5004",
   "metadata": {},
   "source": [
    "## Checking the dataset\n",
    "\n",
    "We will use the same dataset of 146595 molecules used in part 1 but now we will additionally make use of their corresponding $^1$H and $^{13}$C NMR spectra. The following files specify our spectral data:\n",
    "\n",
    "1. <i>CNMR_shifts.p</i> The chemical shifts that we use for the centers of the $^{13}$C NMR spectra bins (80 bins from 3 to 232 ppm). \n",
    "2. <i>HNMR_shifts.p</i> The chemical shifts used to discretize the $^{1}$H NMR spectrum (28000 points from -2 to 12 ppm).\n",
    "3. <i>spectra.h5</i> Contains arrays of shape 28080 for each molecule where the first 28000 values are the normalized intensities of the $^1$H NMR spectrum and the last 80 values are the binned $^{13}$C NMR peaks.\n",
    "\n",
    "Note that we opt to use highly resolved H NMR spectra inputs but coarsely resolved C NMR spectra inputs because the former is richer in information content. More specifically, the coupling between the nuclear spins of neighboring H atoms can lead to the splitting of NMR peaks for those H atoms. Hence, H NMR peak splittings carry valuable information about the neighboring environment around a given H atom. On the other hand, $^{13}$C NMR spectra are typically \"proton decoupled\", which means that the measurement is made in such a way that the coupling of $^{13}$C nuclear spins with neighboring $^{1}$H nuclear spins is averaged out so that usually no C NMR peak splittings are observed. Also, since $^{13}$C is also a relatively rare isotope of C, there will likely not be much $^{13}$C-$^{13}$C coupling eiter.\n",
    "\n",
    "Below we load the data for one of the molecules in our dataset. Plot both the H and C NMR spectra for that molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e126a-3072-4a16-810a-daa30f909906",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10000\n",
    "\n",
    "ismi = np.load('data/smiles.npy')[i].decode('UTF-8')\n",
    "\n",
    "substruct_list = pkl.load((open('data/substructures_957.p', 'rb')))\n",
    "hf = h5py.File('data/substructures.h5', 'r')\n",
    "isubs = hf['substructure_labels'][i]\n",
    "hf.close()\n",
    "\n",
    "cnmr_shifts = pkl.load(open('data/CNMR_shifts.p', 'rb'))\n",
    "hnmr_shifts = pkl.load(open('data/HNMR_shifts.p', 'rb'))\n",
    "hf = h5py.File('data/spectra.h5', 'r')\n",
    "ispectra = hf['spectra'][i]\n",
    "ihnmr = ispectra[:28000]\n",
    "icnmr = ispectra[28000:]\n",
    "hf.close()\n",
    "\n",
    "# Plotting NMR spectra for molecule i\n",
    "### MODIFY BELOW ###\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "axs[0].plot(hnmr_shifts, ihnmr)\n",
    "axs[0].set_xlim(-2, 12)\n",
    "axs[0].set_xlabel('H NMR shift (ppm)')\n",
    "axs[0].set_ylabel('Normalized Intensity')\n",
    "axs[1].plot(cnmr_shifts, icnmr, 'o')\n",
    "axs[1].set_xlim(np.min(cnmr_shifts), np.max(cnmr_shifts))\n",
    "axs[1].set_xlabel('C NMR shift (ppm)')\n",
    "axs[1].set_ylabel('Bin Occupancy')\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.show()\n",
    "### MODIFY ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2743b0f7-e0f2-450e-b60e-6c16791456c9",
   "metadata": {},
   "source": [
    "The following block uses the rdKit software package to visualize the molecule as specified by its SMILES string like we did in part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d0d064-9d26-4d36-913d-0b961b8df955",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ismi)\n",
    "imol = Chem.MolFromSmiles(ismi)\n",
    "Chem.Draw.MolToImage(imol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d80c7e0-7be9-43c2-be19-e31ee2dfa649",
   "metadata": {},
   "source": [
    "## Building our spectra to structure model\n",
    "\n",
    "Since the multi-task model will use the same Transformer and PositionalEncoding classes we developed in part 1, we will simply load the implementation from the NMR2Struct package here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65baf03-f551-473c-9540-1bab76756762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as f\n",
    "from typing import Tuple, Callable, Optional, Any\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from nmr.models import TransformerModel\n",
    "from nmr.networks.encoder import PositionalEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4392c8-a03e-4794-8519-635e84bf7dea",
   "metadata": {},
   "source": [
    "Let us start by working on the part of the model that processes and encodes the inputed NMR spectra. We will process the H NMR spectrum, as specified by an array of normalized NMR intensities for 28000 chemical shift values from -2 to 12 ppm, with 1D convolutional layers interspersed with pooling layers. On the other hand, we will use a smaller input array for the binned C NMR spectrum and hence use a simple embedding scheme like we have done for other components already. \n",
    "\n",
    "Convolutional layers are particularly well-suited for processing image-like data where there are a lot of pixels but information content is localized but has translational symmetry (e.g., identifying whether a large image contains a dog). The H NMR spectra we use here are effectively sparse 1D images with motifs like peak splittings that are local in character, but that can be translationally shifted in ppm depending on the local environment around a specific H atom.\n",
    "\n",
    "We will use the function below without modification to first process our raw spectra inputs (both the H and C NMR spectra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92c26fd-4746-4d6d-9af0-3f93ebaf2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_fwd_fxn_conv_embedding(src: Tensor,\n",
    "                               d_model: int,\n",
    "                               src_embed: nn.Module,\n",
    "                               src_pad_token: int,\n",
    "                               pos_encoder: nn.Module) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    \"\"\"Forward processing for the source tensor where the input is an unprocessed spectrum\n",
    "    This forward function is only to be used with the convolutional source embedding. It has hard-coded values\n",
    "    to allow for all shapes to line up with the current spectrum representation\n",
    "    Args:\n",
    "        src: The unembedded source tensor, in this case representing a spectrum, (batch_size, seq_len)\n",
    "        d_model: The dimensionality of the model\n",
    "        src_embed: The source embedding layer\n",
    "        src_pad_token: The source padding token index. In the case of this forward function, it is \n",
    "            hard-coded to 0\n",
    "        pos_encoder: The positional encoder layer\n",
    "    \"\"\"\n",
    "    assert(src_embed is not None)\n",
    "    #Only construct cnmr padding mask if using cnmr information\n",
    "    if src_embed.use_cnmr:\n",
    "        cnmr_start = src_embed.n_spectral_features\n",
    "        cnmr_end = cnmr_start + src_embed.n_Cfeatures\n",
    "        cnmr = src[:, cnmr_start:cnmr_end]\n",
    "        assert(cnmr.shape[-1] == src_embed.n_Cfeatures)\n",
    "        sorted_cnmr = torch.sort(cnmr, dim = -1, descending=True).values\n",
    "        cnmr_key_pad_mask = (sorted_cnmr == 0).bool().to(src.device)\n",
    "    else:\n",
    "        cnmr_key_pad_mask = torch.tensor([]).bool().to(src.device)\n",
    "\n",
    "    if src_embed.use_hnmr:\n",
    "        src_key_pad_mask = torch.zeros(src.shape[0], \n",
    "                                    src_embed.h_spectrum_final_seq_len).bool().to(src.device)\n",
    "    else:\n",
    "        src_key_pad_mask = torch.tensor([]).bool().to(src.device)\n",
    "    \n",
    "    #Concatenate the padding masks together\n",
    "    src_key_pad_mask = torch.cat((src_key_pad_mask, cnmr_key_pad_mask), dim = -1)\n",
    "    \n",
    "    src_embedded = src_embed(src) * math.sqrt(d_model)\n",
    "    if src_embed.add_pos_encoder:\n",
    "        src_embedded = pos_encoder(src_embedded, None)\n",
    "    return src_embedded, src_key_pad_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa9de59-6ba7-4b13-9106-1cb8039cc6bb",
   "metadata": {},
   "source": [
    "Now we will construct the part of the model that will perform the embeddings of both the H and C NMR spectra. You are asked in particular to complete the initialization for the ConvolutionalEmbedding class below as well as the _embed_cnmr, _embed_spectra, and forward methods.\n",
    "\n",
    "For the initialization, please see the specific comments for what built-in Pytorch nn layer should be initialized.\n",
    "\n",
    "In the _embed_spectra method you will want to chain together the appropriate layers that were initialzed upon the creation of a ConvolutionalEmbedding object in ordering as is described on the righthand side of Figure 3 for embedding inputted H NMR spectra.\n",
    "\n",
    "In _embed_cnmr, to match the architecture of the model we trained, you will want to transform the inputted C NMR spectrum from its binary binned representation to a zero-padded array that lists in order of index which C NMR bins were occupied (Hint: this is almost the same thing you did for the substructure array inputs in part 1!). You will then want to pass the 1-indexed representation of the C NMR data through an embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9437be60-82f3-400c-8bec-2e897283705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 n_hnmr_features: int = 28000,\n",
    "                 n_cnmr_features: int = 40,\n",
    "                 pool_variation: str = 'max',\n",
    "                 pool_size_1: int = 12,\n",
    "                 out_channels_1: int = 64,\n",
    "                 kernel_size_1: int = 5,\n",
    "                 pool_size_2: int = 20,\n",
    "                 out_channels_2: int = 128,\n",
    "                 kernel_size_2: int = 9,\n",
    "                 add_pos_encode: bool = True,\n",
    "                 use_hnmr: bool = True,\n",
    "                 use_cnmr: bool = True):\n",
    "        \"\"\"Construct features over the spectrum using the same convolutional heads as \n",
    "        the convolutional neural network. The convolutional head involves \n",
    "        two 1D convolutions interspersed with max pooling. The channel dimensionalities\n",
    "        are tunable, but default is out_channels_one = 64, out_channels_two = 128. \n",
    "        Convolution strides are 1, padding is 'valid' (no padding), and the activation\n",
    "        function is ReLU. \n",
    "\n",
    "        Args:\n",
    "            d_model: Model dimensionality for downstream transformer\n",
    "            n_hnmr_features: The number of hnmr features, defaults to 28000\n",
    "            n_cnmr_features: The number of cnmr features, defaults to 40\n",
    "            pool_variation: The type of pooling to use, either 'max' or 'avg' where\n",
    "                'max' is max pooling and 'avg' is average pooling, both 1D variants\n",
    "            pool_size_1/2: Size and stride for the respective max pooling layer\n",
    "            out_channels_1/2: Number of output channels after the respective convolutional layer\n",
    "            kernel_size_1/2: Kernel size for the respective convolutional layer\n",
    "            add_pos_encode: Whether to add a positional encoding to the output of this source \n",
    "                embedding.\n",
    "            use_hnmr: Whether HNMR information is used by the network, defaults to True\n",
    "            use_cnmr: Whether CNMR information is used by the network, defaults to True\n",
    "\n",
    "        Notes: \n",
    "            Original architectures:\n",
    "                conv1: Kernel size = 5, Filters (out channels) = 64, in channels = 1\n",
    "                pool1: Max pool of size 12 with stride 12\n",
    "                conv2: Kernel size of 9, Filters (out channels) = 128, in channels = 64\n",
    "                pool2: Max pool of size 20 with stride 20\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_spectral_features = n_hnmr_features\n",
    "        self.n_Cfeatures = n_cnmr_features\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        ### MODIFY BELOW ###\n",
    "\n",
    "        ### MODIFY ABOVE ###\n",
    "        \n",
    "        self.h_spectrum_final_seq_len = self._compute_final_seq_len(\n",
    "            self.n_spectral_features,\n",
    "            [(kernel_size_1, pool_size_1, pool_variation), \n",
    "             (kernel_size_2, pool_size_2, pool_variation)]\n",
    "        )\n",
    "        self.add_pos_encoder = add_pos_encode\n",
    "        self.use_hnmr = use_hnmr\n",
    "        self.use_cnmr = use_cnmr\n",
    "        #Have to use at least one source of spectral information as input\n",
    "        #   to the model!\n",
    "        assert self.use_hnmr or self.use_cnmr\n",
    "\n",
    "        print(\"Final sequence length after conv embedding:\")\n",
    "        print(self.h_spectrum_final_seq_len)\n",
    "    \n",
    "    #From https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "    def _calculate_dim_after_conv(self, \n",
    "                                  L_in: int,\n",
    "                                  kernel: int,\n",
    "                                  padding: int,\n",
    "                                  dilation: int,\n",
    "                                  stride: int) -> int:\n",
    "        numerator = L_in + (2 * padding) - (dilation * (kernel - 1)) - 1\n",
    "        return math.floor(\n",
    "            (numerator/stride) + 1\n",
    "        )\n",
    "    #From https://pytorch.org/docs/stable/generated/torch.nn.AvgPool1d.html\n",
    "    # and https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html\n",
    "    def _calculate_dim_after_pool(self,\n",
    "                                  pool_variation: str,\n",
    "                                  L_in: int,\n",
    "                                  kernel: int,\n",
    "                                  padding: int,\n",
    "                                  dilation: int,\n",
    "                                  stride: int) -> int:\n",
    "        if pool_variation == 'max':\n",
    "            numerator = L_in + (2 * padding) - (dilation * (kernel - 1)) - 1\n",
    "            return math.floor(\n",
    "                (numerator/stride) + 1\n",
    "            )\n",
    "        elif pool_variation == 'avg':\n",
    "            numerator = L_in + (2 * padding) - kernel\n",
    "            return math.floor(\n",
    "                (numerator/stride) + 1\n",
    "            )\n",
    "    \n",
    "    def _compute_final_seq_len(self,\n",
    "                               L_in: int,\n",
    "                               block_args: list[tuple[int]]) -> int:\n",
    "        '''Computes the final sequence after a series of convolution + pooling operations\n",
    "        Args:\n",
    "            L_in: The initial sequence length\n",
    "            block_args: A list of tuples, each containing the:\n",
    "                convolution kernel\n",
    "                pool kernel\n",
    "                pooling_variation\n",
    "        \n",
    "        This function assumes:\n",
    "            padding = 0\n",
    "            dilation = 1\n",
    "            stride = 1 for conv, stride = pool_size for pool\n",
    "        '''\n",
    "        L_final = L_in\n",
    "        for conv_kernel, pool_kernel, pool_variation in block_args:\n",
    "            L_final = self._calculate_dim_after_conv(L_final, conv_kernel, 0, 1, 1)\n",
    "            L_final = self._calculate_dim_after_pool(pool_variation, L_final, pool_kernel, 0, 1, pool_kernel)\n",
    "        return L_final\n",
    "\n",
    "    def _separate_spectra_components(self, x: Tensor):\n",
    "        if len(x.shape) == 2:\n",
    "            x = torch.unsqueeze(x, 1)\n",
    "        spectral_x = x[:, :, :self.n_spectral_features]\n",
    "        cnmr_x = x[:, :, self.n_spectral_features:self.n_spectral_features + self.n_Cfeatures]\n",
    "        mol_x = x[:, :, self.n_spectral_features + self.n_Cfeatures:]\n",
    "        return spectral_x, cnmr_x, mol_x\n",
    "    \n",
    "    def _embed_cnmr(self, cnmr: Tensor):\n",
    "        \"\"\"Embeds the binary tensor into a continuous space\n",
    "        Convert to 1-indexed indices, pad with 0\n",
    "        \"\"\"\n",
    "        assert(cnmr.shape[-1] == self.n_Cfeatures)\n",
    "        if cnmr.ndim == 3:\n",
    "            cnmr = cnmr.squeeze(1)\n",
    "        padder_idx = self.n_Cfeatures * 2\n",
    "        ### MODIFY BELOW ###\n",
    "\n",
    "        ### MODIFY ABOVE ###\n",
    "\n",
    "    def _embed_spectra(self, spectra: Tensor):\n",
    "        assert spectra.ndim == 3\n",
    "        ### MODIFY BELOW ###\n",
    "\n",
    "        ### MODIFY ABOVE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        spectra, cnmr, mol = self._separate_spectra_components(x)\n",
    "        ### MODIFY BELOW ###\n",
    "\n",
    "        ### MODIFY ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9c0db9-78dd-4d57-aaea-46b0952c9047",
   "metadata": {},
   "source": [
    "For the Transformer Encoder part of the model that takes the embedded spectral data and outputs the predicted substructure probabilities, most of the code has been provided below since the architecture is similar to that of the Encoder-Decoder Transformer you already built but with just the Encoder part. The only part that you need to complete is the implementation of the final feedforward layer, which is basically a single layer neural network with a linear layer followed with the application of sigmoid activation functions, as defined by the SingleLinear class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc6076d-527d-4d0a-a830-a85f68dd531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_fwd_fxn_packed_tensor(src: tuple[Tensor],\n",
    "                              d_model: int,\n",
    "                              src_embed: nn.Module,\n",
    "                              src_pad_token: int,\n",
    "                              pos_encoder: nn.Module) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    \"\"\" Forward processing for a source tensor that is a tuple which contains \n",
    "    the embedded sequence and the padding mask\"\"\"\n",
    "    assert(src_embed is None)\n",
    "    src_embedded, src_key_pad_mask = src\n",
    "    return src_embedded, src_key_pad_mask\n",
    "\n",
    "\n",
    "class SeqPool(nn.Module):\n",
    "    \"\"\"Sequence pooling operation introduced in https://arxiv.org/abs/2104.05704\"\"\"\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.g = nn.Linear(d_model, 1)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: (bsize, seq_len, d_model) -> (bsize, d_model)\n",
    "        Double check this code: https://github.com/SHI-Labs/Compact-Transformers/blob/main/src/utils/transformers.py\n",
    "        \"\"\"\n",
    "        x_p = f.softmax(self.g(x), dim = 1)\n",
    "        z = torch.bmm(x_p.transpose(1, 2), x)\n",
    "        return z.squeeze(1)\n",
    "\n",
    "\n",
    "class SingleLinear(nn.Module):\n",
    "    \"\"\"A single linear layer with a sigmoid activation\"\"\"\n",
    "    def __init__(self, d_model: int, d_out: int):\n",
    "        ### MODIFY BELOW ###\n",
    "\n",
    "        ### MODIFY ABOVE ###\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        ### MODIFY BELOW ###\n",
    "\n",
    "        ### MODIFY ABOVE ###\n",
    "\n",
    "\n",
    "class EncoderNetwork(nn.Module):\n",
    "\n",
    "    model_id = 'EncoderNet'\n",
    "\n",
    "    def __init__(self, \n",
    "                 src_embed: nn.Module,\n",
    "                 src_pad_token: int,\n",
    "                 src_forward_function: Callable[[Tensor, nn.Module, int, Optional[nn.Module]], tuple[Tensor, Optional[Tensor]]],\n",
    "                 pooler: nn.Module,\n",
    "                 pooler_opts: dict, \n",
    "                 output_head: nn.Module,\n",
    "                 output_head_opts: dict,\n",
    "                 d_model: int = 512,\n",
    "                 nhead: int = 8,\n",
    "                 dim_feedforward: int = 2048,\n",
    "                 d_out: int = 957,\n",
    "                 source_size: int = 957,\n",
    "                 dropout: float = 0.1,\n",
    "                 activation: str = 'relu',\n",
    "                 layer_norm_eps: float = 1e-05,\n",
    "                 batch_first: bool = True,\n",
    "                 enable_norm: bool = True,\n",
    "                 norm_first: bool = False,\n",
    "                 bias: bool = True,\n",
    "                 num_layers: int = 6,\n",
    "                 enable_nested_tensor: bool = True,\n",
    "                 device: torch.device = None,\n",
    "                 dtype: torch.dtype = torch.float\n",
    "                 ):\n",
    "        r\"\"\"Most parameters are for the PyTorch TransformerEncoderLayer module.\n",
    "        Args:\n",
    "            src_embed: The embedding module for the src tensor passed to the model\n",
    "            src_pad_token: The index used to indicate padding in the source sequence\n",
    "            src_forward_function: A function that processes the src tensor using the src embedding, src pad token, and positional encoding to generate\n",
    "                the embedded src and the src_key_pad_mask\n",
    "            pooler: Module for pooling the output of the transformer. This is typically seen in sequence classification \n",
    "                tasks where pooling is applied in the sequence length/time dimension, i.e.:\n",
    "                (N, T, E) --> (N, E) where N = batch size, T = seq len, and E = feature dimension.\n",
    "                The pooler should take in a 3D input of shape (N, T, E) and produce an output of shape (N, E)\n",
    "            pooler_opts: Additional options for the pooling head\n",
    "            output_head: Module for generating the output from the pooled transformer output. \n",
    "                This module should take at minimum a 2D input of shape (N, E) and produce an output of shape (N, d_out)\n",
    "            output_head_opts: Additional options for the output head\n",
    "            d_model: The dimensionality of the model\n",
    "            nhead: The number of heads in the multiheadattention models\n",
    "            dim_feedforward: The inner dimension of the feedforward network model\n",
    "            d_out: The dimensionality of the output (e.g. 957 for the number of substructures)\n",
    "            source_size: The size of the source vocabulary, \n",
    "            batch_first: If True, then all tensors are shaped as (N, T, E), with N being batch size\n",
    "            enable_norm: If True, then layer normalization is applied as in the original transformer encoder + decoder.\n",
    "                Default is True\n",
    "            num_layers: The number of layers to use in the encoder\n",
    "            enable_nested_tensor: Whether nested tensor operations are enabled inside the transformer. By default enabled,\n",
    "                improves performance when padding is high\n",
    "            device: The device for the model\n",
    "            dtype: The dtype for the model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.src_embed = src_embed\n",
    "        self.src_size = source_size\n",
    "        self.src_fwd_fn = src_forward_function\n",
    "        self.src_pad_token = src_pad_token\n",
    "        self.d_model = d_model\n",
    "        self.d_out = d_out\n",
    "        self.nhead = nhead\n",
    "        self.nlayers = num_layers\n",
    "\n",
    "        self.pooler = pooler(**pooler_opts)\n",
    "        self.output_head = output_head(**output_head_opts)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        #Construct the encoder model. Process taken from \n",
    "        #   https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            layer_norm_eps=layer_norm_eps,\n",
    "            batch_first=batch_first,\n",
    "            norm_first=norm_first,\n",
    "            bias=bias,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "        if enable_norm:\n",
    "            norm = nn.LayerNorm(d_model, eps=layer_norm_eps, bias=bias, device=device, dtype=dtype)\n",
    "        else:\n",
    "            norm = None\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, \n",
    "            num_layers=num_layers, \n",
    "            norm=norm,\n",
    "            enable_nested_tensor=enable_nested_tensor\n",
    "        )\n",
    "    \n",
    "    def _sanitize_forward_args(self, x: tuple[Tensor, tuple[str]]) -> Tensor:\n",
    "        x, _ = x\n",
    "        if isinstance(self.src_embed, nn.Embedding):\n",
    "            x = x.long()\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x: tuple[Tensor, tuple[str]]) -> Tensor:\n",
    "        src = self._sanitize_forward_args(x)\n",
    "        src_embedded, src_key_pad_mask = self.src_fwd_fn(src, self.d_model, self.src_embed, self.src_pad_token, self.pos_encoder)\n",
    "        src_out = self.encoder(src=src_embedded,\n",
    "                               src_key_padding_mask=src_key_pad_mask)\n",
    "        src_out = self.pooler(src_out)\n",
    "        return self.output_head(src_out)\n",
    "    \n",
    "    def get_loss(self, \n",
    "                 x: tuple[Tensor, tuple[str]],\n",
    "                 y: tuple[Tensor],\n",
    "                 loss_fn: Callable[[Tensor, Tensor], Tensor]) -> Tensor:\n",
    "        pred = self.forward(x)\n",
    "        y_target, = y\n",
    "        loss = loss_fn(pred, y_target.to(self.dtype).to(self.device))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class EncoderModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 src_embed: str,\n",
    "                 src_embed_options: dict,\n",
    "                 src_pad_token: int, \n",
    "                 src_forward_function: str,\n",
    "                 pooler: str,\n",
    "                 pooler_opts: dict,\n",
    "                 output_head: str,\n",
    "                 output_head_opts: dict,\n",
    "                 d_model: int,\n",
    "                 nhead: int = 8,\n",
    "                 dim_feedforward: int = 2048,\n",
    "                 d_out: int = 957,\n",
    "                 source_size: int = 957,\n",
    "                 dropout: float = 0.1,\n",
    "                 activation: str = 'relu',\n",
    "                 layer_norm_eps: float = 1e-05,\n",
    "                 batch_first: bool = True,\n",
    "                 enable_norm: bool = True,\n",
    "                 norm_first: bool = False,\n",
    "                 bias: bool = True,\n",
    "                 num_layers: int = 6,\n",
    "                 enable_nested_tensor: bool = True,\n",
    "                 freeze_components: Optional[list] = None,\n",
    "                 device: torch.device = None,\n",
    "                 dtype: torch.dtype = torch.float\n",
    "                 ):\n",
    "        r\"\"\"See documentation for EncoderNetwork for description of parameters. The parameters replaced by\n",
    "        string values are meant to be names of the compnents to be fetched using getattr\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        src_embed_layer = None\n",
    "        pooler = SeqPool\n",
    "        output_head = SingleLinear\n",
    "\n",
    "        src_fwd_fn = src_fwd_fxn_packed_tensor\n",
    "        self.network = EncoderNetwork(\n",
    "            src_embed=src_embed_layer,\n",
    "            src_pad_token=src_pad_token,\n",
    "            src_forward_function=src_fwd_fn,\n",
    "            pooler=pooler,\n",
    "            pooler_opts=pooler_opts,\n",
    "            output_head=output_head,\n",
    "            output_head_opts=output_head_opts,\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            d_out=d_out,\n",
    "            source_size=source_size,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            layer_norm_eps=layer_norm_eps,\n",
    "            batch_first=batch_first,\n",
    "            enable_norm=enable_norm,\n",
    "            norm_first=norm_first,\n",
    "            bias=bias,\n",
    "            num_layers=num_layers,\n",
    "            enable_nested_tensor=enable_nested_tensor,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "        self.initialize_weights()\n",
    "        self.freeze_components=freeze_components\n",
    "        self.device=device\n",
    "        self.dtype=dtype\n",
    "    \n",
    "    def initialize_weights(self) -> None:\n",
    "        '''Initializes network weights'''\n",
    "        for p in self.network.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def freeze(self) -> None:\n",
    "        '''Disables greadients for specific network components'''\n",
    "        if self.freeze_components is not None:\n",
    "            for component in self.freeze_components:\n",
    "                if hasattr(self.network, component):\n",
    "                    for param in getattr(self.network, component).parameters():\n",
    "                        param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x: tuple[Tensor, tuple[str]]) -> Tensor:\n",
    "        return self.network(x)\n",
    "    \n",
    "    def get_loss(self, \n",
    "                 x: tuple[Tensor, tuple[str]],\n",
    "                 y: tuple[Tensor],\n",
    "                 loss_fn: Callable[[Tensor, Tensor], Tensor]) -> Tensor:\n",
    "        return self.network.get_loss(x, y, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f51f1-1cfa-4d90-a407-135c36e5bff8",
   "metadata": {},
   "source": [
    "For the last step, let us connect all the separate parts together to finalize the implementaion of the multi-task model. To do that, complete the indicated parts of initialization and forward methods for the MultiTaskModel class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012874c8-6276-4663-80a3-3379d07e62bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    \"\"\" Model that predicts substructures and structures \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 src_embed: str,\n",
    "                 src_embed_options: dict,\n",
    "                 structure_model: str,\n",
    "                 structure_model_args: dict,\n",
    "                 substructure_model: str,  \n",
    "                 substructure_model_args: dict,\n",
    "                 forward_fxn: str, \n",
    "                 structure_model_ckpt: str,\n",
    "                 substructure_model_ckpt: str,\n",
    "                 device: torch.device = None,\n",
    "                 dtype: torch.dtype = torch.float):\n",
    "        \"\"\"Constructor for multitask model that takes in a src embedding used to produce a structure and substructure prediction\"\"\"\n",
    "        super().__init__()\n",
    "        self.src_embed = ConvolutionalEmbedding(**src_embed_options)\n",
    "        self.structure_model = TransformerModel(**structure_model_args)\n",
    "        self.substructure_model = EncoderModel(**substructure_model_args)\n",
    "        src_embed_dim = self.src_embed.d_model\n",
    "        structure_model_dim = self.structure_model.network.d_model\n",
    "        substructure_model_dim = self.substructure_model.network.d_model\n",
    "        \n",
    "        #Connecting linear transformations\n",
    "        ### MODIFY BELOW ###\n",
    "\n",
    "        ### MODIFY ABOVE ###\n",
    "\n",
    "        self.fwd_fn = src_fwd_fxn_conv_embedding\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.initialize_weights()\n",
    "        if structure_model_ckpt is not None:\n",
    "            self._partial_load_weights(self.structure_model, structure_model_ckpt)\n",
    "        if substructure_model_ckpt is not None:\n",
    "            self._partial_load_weights(self.substructure_model, substructure_model_ckpt)\n",
    "\n",
    "    def initialize_weights(self) -> None:\n",
    "        \"\"\"initialize network weights\"\"\"\n",
    "        self.structure_model.initialize_weights()\n",
    "        self.substructure_model.initialize_weights()\n",
    "    \n",
    "    def freeze(self) -> None:\n",
    "        \"\"\"Disables gradients for specific components of the network\"\"\"\n",
    "        self.structure_model.freeze()\n",
    "        self.substructure_model.freeze()\n",
    "\n",
    "    def _partial_load_weights(self, model: nn.Module, ckpt: str) -> None:\n",
    "        ckpt = torch.load(ckpt, map_location = self.device)['model_state_dict']\n",
    "        model_state = model.state_dict()\n",
    "        pretrained_dictionary = {}\n",
    "        for k, v in ckpt.items():\n",
    "            if k in model_state:\n",
    "                if model_state[k].shape == v.shape:\n",
    "                    pretrained_dictionary[k] = v\n",
    "                else:\n",
    "                    warnings.warn(f\"Could not load {k}: expected {model_state[k].shape} but got {v.shape}\")\n",
    "            else:\n",
    "                warnings.warn(f\"Could not load {k} because it is not in the model state dictionary\")\n",
    "        print(\"The following keys are ignored in the model:\")\n",
    "        for k in model_state:\n",
    "            if k not in pretrained_dictionary:\n",
    "                print(k)\n",
    "        model.load_state_dict(pretrained_dictionary, strict=False)\n",
    "    \n",
    "    def _sanitize_forward_args(self, x, y):\n",
    "        inp, _ = x\n",
    "        structure_targets, substructure_targets = y\n",
    "        return inp, structure_targets, substructure_targets\n",
    "    \n",
    "    def _unpack_to_list(self, x: Tensor, dim: int) -> list[Tensor]:\n",
    "        \"\"\"\n",
    "        Given a tensor of elements, unpacks the tensor into a tuple of tensors. For exasmple,\n",
    "        a tensor of shape (N, 2, E) -> ((N, E), (N, E))\n",
    "        \"\"\"\n",
    "        return [torch.select(x, dim, i) for i in range(x.shape[dim])]\n",
    "\n",
    "    def forward(self, \n",
    "                x: Tuple[Tensor, Tuple], \n",
    "                y: Tuple[Tensor, Tensor],\n",
    "                eval_paths: list[str]) -> Tensor:\n",
    "        \"\"\"\n",
    "        The argument eval_paths indicates which submodels to evaluate. It can contain the following values:\n",
    "            'structure': The structure model is evaluated on this forward pass\n",
    "            'substructure': The substructure model is evaluated on this forward pass\n",
    "        Note that the forward() function is not used in get_loss. This is because get_loss will always \n",
    "        evaluate both submodels. If one wishes to only use one submodel, they should train the models \n",
    "        using the convolutional embedding instead. \n",
    "        \"\"\"\n",
    "        src, struct_targs, substruct_targs = self._sanitize_forward_args(x, y)\n",
    "        if 'structure' in eval_paths:\n",
    "            ### MODIFY BELOW ###\n",
    "\n",
    "            ### MODIFY ABOVE ###\n",
    "        else:\n",
    "            structure_output = None\n",
    "        if 'substructure' in eval_paths:\n",
    "            ### MODIFY BELOW ###\n",
    "\n",
    "            ### MODIFY ABOVE ###\n",
    "        else:\n",
    "            substructure_output = None\n",
    "        return structure_output, substructure_output\n",
    "    \n",
    "    def get_loss(self,\n",
    "                 x: Tuple[Tensor, Tuple], \n",
    "                 y: Tuple[Tensor], \n",
    "                 loss_fn: Callable[[Tensor, Tensor], Tensor]) -> Tensor:\n",
    "        structure_loss = lambda x, y : loss_fn('structure', x, y)\n",
    "        substructure_loss = lambda x, y : loss_fn('substructure', x, y)\n",
    "        src, struct_targs, substruct_targs = self._sanitize_forward_args(x, y)\n",
    "        src_struct_embedded, src_struct_key_pad_mask = self.fwd_fn(src,\n",
    "                                                                   self.structure_model.network.d_model, \n",
    "                                                                   self.src_embed, \n",
    "                                                                   self.structure_model.network.src_pad_token, \n",
    "                                                                   self.structure_model.network.pos_encoder)\n",
    "        src_substruct_embedded, src_substruct_key_pad_mask = self.fwd_fn(src,\n",
    "                                                                       self.substructure_model.network.d_model, \n",
    "                                                                       self.src_embed, \n",
    "                                                                       self.substructure_model.network.src_pad_token, \n",
    "                                                                       self.substructure_model.network.pos_encoder)\n",
    "        src_struct_embedded = self.structure_connector(src_struct_embedded)\n",
    "        src_substruct_embedded = self.substructure_connector(src_substruct_embedded)\n",
    "        #Loss scaling factors are multiplied within forward() calls\n",
    "        structure_loss = self.structure_model.get_loss(((src_struct_embedded, src_struct_key_pad_mask), None), \n",
    "                                                       self._unpack_to_list(struct_targs, 1), \n",
    "                                                       structure_loss)\n",
    "        substructure_loss = self.substructure_model.get_loss(((src_substruct_embedded, src_substruct_key_pad_mask), None),\n",
    "                                                              (substruct_targs,), substructure_loss)\n",
    "        return structure_loss + substructure_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d925117-ce0f-45cd-9778-342e0c7fe77c",
   "metadata": {},
   "source": [
    "## Loading our multi-task model\n",
    "\n",
    "While you were implementing your model, hopefully your multi-task model has also been training. If it has trained to a point where the loss over the validation set has seemingly plateaud, copy the contents of the corresponding \"checkpoints\" subfolder into a folder in the directory here named \"model2\". If training has not seemingly saturated, you can instead use a provided trained model (see \"multitask_checkpoint.pt\" provided by the NMR2Struct package [here](https://github.com/MarklandGroup/NMR2Struct/tree/main/checkpoints)).\n",
    "\n",
    "Below we will try to load the trained model using the code you implemented in this notebook, which is a stripped down reimplementation of the relevant code from the NMR2Struct package that we used to train this model. If you receive an error here, you will likely need to debug your implementation of the model in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824106e-f484-4095-aa9d-27daf20d9162",
   "metadata": {},
   "outputs": [],
   "source": [
    "listdoc =  yaml.safe_load(open('model2/full_inference_config.yaml', 'r'))\n",
    "model_args = listdoc['model']\n",
    "model_config = model_args['model_args']\n",
    "\n",
    "model = MultiTaskModel(dtype=torch.float32, device=torch.device('cpu'), **model_config)\n",
    "ckpt = torch.load('model2/multitask_checkpoint.pt', map_location=torch.device('cpu'))['model_state_dict']\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc8e292-9b83-4099-9ef9-195a11ba806b",
   "metadata": {},
   "source": [
    "## Inference with our multi-task model\n",
    "\n",
    "With the model loaded, let us now attempt to use it to try predict both the substructures and the full structure of a molecule given only the corresponding pair of H and C NMR spectra for the molecule. Below we load the SMILES string, substructure list, and concatenated H and C NMR spectra for one of the molecules in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4cf0a-81d4-41d9-a552-90b81f93dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10000\n",
    "\n",
    "ismi = np.load('data/smiles.npy')[i].decode('UTF-8')\n",
    "\n",
    "substruct_list = pkl.load((open('data/substructures_957.p', 'rb')))\n",
    "hf = h5py.File('data/substructures.h5', 'r')\n",
    "isubs = hf['substructure_labels'][i]\n",
    "hf.close()\n",
    "hf = h5py.File('data/spectra.h5', 'r')\n",
    "ispectra = hf['spectra'][i]\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1891cb2-6bfd-4850-a2dc-aefcebcfd601",
   "metadata": {},
   "source": [
    "Complete the code block below to pass the spectra loaded above to our multi-task model to make a prediction for the first character of the molecule's SMILES string given the list of substructures for that molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb5f63-13b6-45f0-a5a0-2e9da66c4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = np.load('model2/alphabet.npy')\n",
    "start_token = 22\n",
    "stop_token = 23\n",
    "\n",
    "### MODIFY BELOW ###\n",
    "\n",
    "### MODIFY ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914601fa-3a31-4d76-a5e4-edf3eac7e566",
   "metadata": {},
   "source": [
    "To roughly get a picture for how accurate our spectra to substructure model is, plot below the difference between the predicted substructure probabilities and the actual substructure labels for this molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca8603-652e-4b9a-8e7e-6d168a05ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFY BELOW ###\n",
    "\n",
    "### MODIFY ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98105ef6-f9ba-4cbc-a577-2ecbe558f6b0",
   "metadata": {},
   "source": [
    "Below, identify and visualize (remember rdKit?) which substructure predictions were false positives, which we will define here as substructures not a part of the molecule having a predicted probability of greater than 0.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7e1b2-93a5-4982-abb7-7e23950ced09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFY BELOW ###\n",
    "\n",
    "### MODIFY ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb343a-12a6-48d8-9c8a-dd6daafbc0a9",
   "metadata": {},
   "source": [
    "Below, identify and visualize which substructure predictions were false negatives, which we will define here as substructures contained by the molecule having a predicted probability of less than 0.4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e4686c-21fa-4314-8389-923bd8d1a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFY BELOW ###\n",
    "\n",
    "### MODIFY ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6026f363-5b56-4166-be39-1de48b47cfcf",
   "metadata": {},
   "source": [
    "Complete the generate_structure function below so that it autoregressively predicts for a molecular structure given an input array that is concatenation of the H and C NMR spectra. As you might have guessed, it will look quite similar to what you implemented in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35184d-af3a-44f6-aade-db0e6d6672b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_sample_batched(k_val: int | float , \n",
    "                             character_probabilities: Tensor) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Generates the next character using top-k sampling scheme.\n",
    "\n",
    "    In top-k sampling, the probability mass is redistributed among the\n",
    "    top-k next tokens, where k is a hyperparameter. Once redistributed, \n",
    "    the next token is sampled from the top-k tokens.\n",
    "    \"\"\"\n",
    "    top_values, top_indices = torch.topk(character_probabilities, k_val, sorted = True)\n",
    "    #Take the sum of the top probabilities and renormalize\n",
    "    tot_probs = top_values / torch.sum(top_values, dim = -1).reshape(-1, 1)\n",
    "    #Sample from the top k probabilities. This represents a multinomial distribution\n",
    "    try:\n",
    "        assert(torch.allclose(torch.sum(tot_probs, dim = -1), torch.tensor(1.0)))\n",
    "    except:\n",
    "        print(\"Probabilities did not pass allclose check!\")\n",
    "        print(f\"Sum of probs is {torch.sum(tot_probs)}\")\n",
    "    selected_index = torch.multinomial(tot_probs, 1)\n",
    "    #For gather to work, both tensors have to have the same number of dimensions:\n",
    "    if len(top_indices.shape) != len(selected_index.shape):\n",
    "        top_indices = top_indices.reshape(selected_index.shape[0], -1)\n",
    "    output = torch.gather(top_indices, -1, selected_index)\n",
    "    output_token_probs = torch.gather(tot_probs, -1, selected_index)\n",
    "    return output, output_token_probs\n",
    "    \n",
    "\n",
    "def generate_structure(model, ispectra, start_token=22, stop_token=23, max_len=74, sample_val=5, max_steps=100):\n",
    "    ### MODIFY BELOW ###\n",
    "\n",
    "    ### MODIFY ABOVE ###\n",
    "    \n",
    "\n",
    "pred_tokens, pred_token_probs = generate_structure(model, ispectra)\n",
    "pred_smi = ''\n",
    "for ichar in alphabet[pred_tokens[0,1:-1]]:\n",
    "    pred_smi+=ichar\n",
    "print('Target: ' + ismi)\n",
    "print('Predicted: ' + pred_smi)\n",
    "print('Predicted Score: ' + str(np.log(pred_token_probs[0,:-1].detach().numpy()).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56952f2b-3972-4300-9150-6d7732c23698",
   "metadata": {},
   "source": [
    "Again just like in part 1, write a routine that samples 10 molecule predictions, saving for each molecule its SMILES string and its log probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b63bb5c-9972-425c-bf2b-45e202d8ac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_k_structures(model, ispectra, num_pred_per_tgt):\n",
    "    ### MODIFY BELOW ###\n",
    "\n",
    "    ### MODIFY ABOVE ###\n",
    "\n",
    "\n",
    "sampled_smis, sampled_smi_scores = generate_k_structures(model, ispectra, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e0b5f4-95bc-471c-a671-93c7a199f1c5",
   "metadata": {},
   "source": [
    "Use the below code block to visualize the 10 predicted molecules as ordered by their log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf33e2-8a6c-4f9d-b6bb-599f232f053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_mols = [Chem.MolFromSmiles(x) for x in sampled_smis]\n",
    "Chem.Draw.MolsToGridImage(sampled_mols,molsPerRow=5,subImgSize=(200,200), legends=[str(x) for x in sampled_smis])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f760ccde-99c4-498e-b46a-561375278c4a",
   "metadata": {},
   "source": [
    "Now you have a fully working mulit-task model that can be used to make spectra-to-substructure and even spectra-to-structure predictions, and all in just an afternoon's work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48395a-0a32-40fc-b9d2-23e546e077cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NMR_env",
   "language": "python",
   "name": "nmr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
