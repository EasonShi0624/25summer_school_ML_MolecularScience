{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abcb21da-fcb1-47b1-b9dc-5cc6af83d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa570f6-3869-4abe-af00-53f69c4a5c2b",
   "metadata": {},
   "source": [
    "# Molecular structure elucidation from NMR spectra: Part 1\n",
    "\n",
    "In this exercise we will examine one of the applications of the Transformer architecture we covered in lecture today. The example here is based on recently published work by [Hu et. al.](https://pubs.acs.org/doi/10.1021/acscentsci.4c01132) where a machine learning (ML) model was developed to tackle the inverse problem of elucidating molecular structure from routintely collected 1D NMR spectra (see below). If you have taken an organic chemistry class, you likely had to assign peaks in $^1$H and $^{13}$C NMR spectra to functional groups (i.e., substructures) and your experimental colleagues routinely conduct NMR measurements to characterize their synthesized products. NMR spectra serve as useful molecular fingerprints, but, even with a set of tabulated shifts characteristic of different functional groups or databases to cross-search spectra, the task of determining molecular structure from NMR spectra can be difficult for larger molecules.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"figures/toc.jpeg\" width=\"85%\"/>\n",
    "    <figcaption><i>Figure 1: (Top) Overview of our ML model that takes NMR spectra as input to predict for the corresponding molecular structure. (Bottom) Downstream parts of the model are pretrained to predict the molecular structure based on a list of substructures that make up the molecule.</i></figcaption>\n",
    "</div>\n",
    "\n",
    "We will attempt to better inform this structure elucidation task with an ML model. To accomplish this, we will represent our molecular structures with [SMILES](https://chem.libretexts.org/Courses/Fordham_University/Chem1102%3A_Drug_Discovery_-_From_the_Laboratory_to_the_Clinic/05%3A_Organic_Molecules/5.08%3A_Line_Notation_(SMILES_and_InChI)) strings, a character-based representation of a molecular graph, and develop a ML model to predict for each character one-by-one autoregressively. In this way, we are effectively \"translating\" from our NMR spectra to SMILES and will leverage a Transformer in order to do so.\n",
    "\n",
    "Since NMR spectra are relatively expensive to generate/source, we will first develop a transformer-based model that just predicts molecular structure given what substructures are present (i.e., the bottom half of Figure 1). In the next half of this exercise, we will use this model to initialize part of a larger model that goes from spectral data inputs to substructure prediction embeddings that are then passed through the transformer we construct here to ultimately obtain molecular structure predictions. Data to train this substructure-to-structure model is a lot easier to source and label, since there exist large datasets of chemically feasible molecular structures and identifying what substructures a given molecule contains is relatively straightforward. The training of the overall spectra to structure model that we will conduct later will be considerably easier if we can pretrain this part of the model.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"figures/architecture1.png\" width=\"35%\"/>\n",
    "    <figcaption><i>Figure 2: Transformer architecture used for our substructure to structure model</i></figcaption>\n",
    "</div>\n",
    "\n",
    "Figure 2 above summarizes the architecture of the transformer we will construct in the first half of this exercise. In short, it will take as input both an array which will indicate what substructures are present in the molecule and a prefix of SMILES characters to make the prediction for the next character in the predicted SMILES string for the molecule. By repeatedly sampling the model with a growing prefix of SMILES character inputs we, in an autoregressive fashion, can predict for a complete SMILES string for the molecule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c687a-c13e-4fd9-aa10-22df053521e6",
   "metadata": {},
   "source": [
    "## Local installation and setup\n",
    "\n",
    "For this exercise we will train our models using GPU resources on the NYU Shanghai cluster using the NMR2Struct software package while deconstructing the models and performing analyses locally on your personal laptops. This is because training is computationally quite demanding but evaluating a trained model and the analyses we will conduct is not.\n",
    "\n",
    "To install NMR2Struct on your personal laptop please visit the [GitHub repo](https://github.com/MarklandGroup/NMR2Struct/tree/main) and follow the install instructions. Since your laptop might not have a dedicated GPU, please modify the environment.yaml file to install a CPU only version of pytorch (i.e., change \"pytorch::pytorch-cuda=12.1\" to \"pytorch\").\n",
    "\n",
    "The dataset we will use to train and evaluate our model has been taken from this [Zenodo archive](https://zenodo.org/records/13892026). The training of the model you will subsequently perform on the NYU Shanghai cluster will make use of this full dataset. On your personal laptop, you need to only download the \"data\" subfolder provided in the Google Drive for this exercise to the same directory with this Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695dba5-7d49-4676-8e07-c3dba3dcba7e",
   "metadata": {},
   "source": [
    "## Training our substructure to structure model\n",
    "\n",
    "Since the training of our substructure to structure model will take some time, let us first get that started before we dive into the other details. Training our model will be straightforward since we will use the full implementation of the model provided by the NMR2Struct software package. To start training the model:\n",
    "\n",
    "1. Login to the NYU Shanghai cluster and navigate to where you would like to perform the training of the model.\n",
    "\n",
    "```\n",
    "ssh <user>@hpclogin.shanghai.nyu.edu\n",
    "```\n",
    "\n",
    "2. Copy over the pre-prepared training run directory:\n",
    "\n",
    "```\n",
    "rsync -avz --exclude=\"datasets\" --exclude=\"NMR2Struct\" /gpfsnyu/home/mc10050/nmr2mol .\n",
    "```\n",
    "\n",
    "3. (Optional) The settings for specifying the model architecture and training hyperparameters. Feel free to try modifying things (e.g., like the embedding dimension of the model d_model) if you would like to try something aside from the default settings! If you have any questions about what the settings refer to, do not hesitate to ask.\n",
    "  \n",
    "4. Submit the training run to the job scheduler:\n",
    "\n",
    "```\n",
    "cd nmr2mol/exercise_part1/\n",
    "sbatch -J sub2struct-train submit.sh\n",
    "```\n",
    "\n",
    "5. You can monitor the progress training your model by downloading the tfevents file generated to your local machine and using Tensorboard as show below and navigating in your web browser to the indicated URL (e.g., \"http://localhost:6006/\"). Note that the losses used to optimize our model parameters and that are visualized via Tensorboard are the standard cross-entropy losses\n",
    "\n",
    "```\n",
    "rsync -avz \"<user>@hpclogin.shanghai.nyu.edu:~/nmr2mol/exercise_part1/checkpoints/events.out.*\" model1/\n",
    "conda activate NMR_env\n",
    "tensorboard --logdir=model1/\n",
    "\n",
    "```\n",
    "\n",
    "Now you just need to wait for your job to run and your model will then start training, which will take on the order of an hour. In the meantime, let us understand the details of the model we are training by building our own version of it from scratch using Pytorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9cbaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending incremental file list\n",
      "drwxrwxrwx          4,096 2025/07/16 12:41:58 nmr2mol\n",
      "drwxrwxrwx          4,096 2025/07/16 12:49:43 nmr2mol/exercise_part1\n",
      "-rwxrwxrwx            517 2025/07/16 12:37:54 nmr2mol/exercise_part1/conda-env.sh\n",
      "-rwxrwxrwx          1,295 2025/07/13 14:17:05 nmr2mol/exercise_part1/full_train_config.yaml\n",
      "-rwxrwxrwx            183 2025/07/16 12:39:10 nmr2mol/exercise_part1/submit.sh\n",
      "drwxrwxrwx          4,096 2025/07/16 12:34:56 nmr2mol/exercise_part1/data\n",
      "lrwxrwxrwx             51 2025/07/16 12:34:56 nmr2mol/exercise_part1/data/alphabet.npy -> /gpfsnyu/home/mc10050/nmr2mol/datasets/alphabet.npy\n",
      "lrwxrwxrwx             49 2025/07/16 12:34:56 nmr2mol/exercise_part1/data/smiles.npy -> /gpfsnyu/home/mc10050/nmr2mol/datasets/smiles.npy\n",
      "lrwxrwxrwx             54 2025/07/16 12:34:56 nmr2mol/exercise_part1/data/split_indices.p -> /gpfsnyu/home/mc10050/nmr2mol/datasets/split_indices.p\n",
      "lrwxrwxrwx             55 2025/07/16 12:34:56 nmr2mol/exercise_part1/data/substructures.h5 -> /gpfsnyu/home/mc10050/nmr2mol/datasets/substructures.h5\n",
      "drwxrwxrwx          4,096 2025/07/16 13:27:05 nmr2mol/exercise_part2\n",
      "-rwxrwxrwx            517 2025/07/16 12:41:35 nmr2mol/exercise_part2/conda-env.sh\n",
      "-rwxrwxrwx          2,185 2025/07/12 17:25:39 nmr2mol/exercise_part2/full_train_config.yaml\n",
      "-rwxrwxrwx            184 2025/07/16 12:58:24 nmr2mol/exercise_part2/submit.sh\n",
      "drwxrwxrwx          4,096 2025/07/16 12:48:23 nmr2mol/exercise_part2/data\n",
      "lrwxrwxrwx             51 2025/07/16 12:34:56 nmr2mol/exercise_part2/data/alphabet.npy -> /gpfsnyu/home/mc10050/nmr2mol/datasets/alphabet.npy\n",
      "lrwxrwxrwx             49 2025/07/16 12:34:56 nmr2mol/exercise_part2/data/smiles.npy -> /gpfsnyu/home/mc10050/nmr2mol/datasets/smiles.npy\n",
      "lrwxrwxrwx             49 2025/07/16 12:43:07 nmr2mol/exercise_part2/data/spectra.h5 -> /gpfsnyu/home/mc10050/nmr2mol/datasets/spectra.h5\n",
      "lrwxrwxrwx             54 2025/07/16 12:34:56 nmr2mol/exercise_part2/data/split_indices.p -> /gpfsnyu/home/mc10050/nmr2mol/datasets/split_indices.p\n",
      "lrwxrwxrwx             55 2025/07/16 12:34:56 nmr2mol/exercise_part2/data/substructures.h5 -> /gpfsnyu/home/mc10050/nmr2mol/datasets/substructures.h5\n",
      "lrwxrwxrwx             66 2025/07/16 12:48:23 nmr2mol/exercise_part2/data/transformer_checkpoint.ckpt -> /gpfsnyu/home/mc10050/nmr2mol/datasets/transformer_checkpoint.ckpt\n",
      "\n",
      "sent 1,212 bytes  received 2,417 bytes  7,258.00 bytes/sec\n",
      "total size is 5,414  speedup is 1.49\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rsync -avz --exclude=\"datasets\" --exclude=\"NMR2Struct\" /gpfsnyu/home/mc10050/nmr2mol /gpfsnyu/scratch/ys6132/2025_summer_school_ML4MS/day3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a4798e-33b8-4897-be07-2f412498f1fc",
   "metadata": {},
   "source": [
    "## Checking the dataset\n",
    "\n",
    "The dataset we will be working with consists of a total of 146595 molecules with each molecule containing up to 19 heavy atoms. The following files contain the different components of our dataset:\n",
    "\n",
    "1. <i>smiles.npy</i> contains all of the SMILES strings for each molecule.\n",
    "2. <i>substructures_957.p</i> contains all the SMARTS strings defining each of the 957 substructures that we are using for our substructure to structure prediction model. SMARTS are similar to SMILES but are more flexible (e.g., can define coordination of atoms) and hence more well suited for defining our substructures.\n",
    "3. <i>substructures.h5</i> contains the substructure labeling for each molecule.\n",
    "\n",
    "Below we load the SMILES string and substructure labels for one molecule from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10e05138-43c8-4f7b-9020-623642183207",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/smiles_test.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 3\u001b[0m ismi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/smiles_test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)[i]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m substruct_list \u001b[38;5;241m=\u001b[39m pkl\u001b[38;5;241m.\u001b[39mload((\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/substructures_957.p\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m      6\u001b[0m hf \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/substructures_test.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/smiles_test.npy'"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "\n",
    "ismi = np.load('data/smiles_test.npy')[i].decode('UTF-8')\n",
    "\n",
    "substruct_list = pkl.load((open('data/substructures_957.p', 'rb')))\n",
    "hf = h5py.File('data/substructures_test.h5', 'r')\n",
    "isubs = hf['substructure_labels'][i]\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b315b-4f8e-439d-aac8-0294cbb26c16",
   "metadata": {},
   "source": [
    "The following block uses the rdKit software package to visualize the molecule as specified by its SMILES string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe8d988-7247-416f-9467-efa9213e64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ismi)\n",
    "imol = Chem.MolFromSmiles(ismi)\n",
    "Chem.Draw.MolToImage(imol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c36c11f-5680-4362-ae25-b2623592f541",
   "metadata": {},
   "source": [
    "The following block uses the rdKit software package to visualize which of the 957 substructures the molecule above has, with the corresponding SMARTS specifying the substructure printed as well. The dashed lines indicate that the bond type (e.g., single, double, or triple) is unspecified for that pair. The model that we will subsequently construct will take the set of substructures below as input to attempt at predicting the full molecular structure above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684cdaa0-78f3-4d4a-bd8c-0ffa88a28611",
   "metadata": {},
   "outputs": [],
   "source": [
    "isubstruct_list = [Chem.MolFromSmarts(x) for x in substruct_list[isubs]]\n",
    "Chem.Draw.MolsToGridImage(isubstruct_list,molsPerRow=8,subImgSize=(150,150),legends=[str(x) for x in substruct_list[isubs]])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e056c49-856c-4d1d-a06e-30e73881a2a2",
   "metadata": {},
   "source": [
    "## Building our substructure to structure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327e9be-8378-47c6-ac01-49a248f2c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "from typing import Tuple, Callable, Optional, Any\n",
    "from torch import nn, Tensor\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04f0ab6-8b43-4898-bb40-64ee2ea0213b",
   "metadata": {},
   "source": [
    "Our model first processes the input data, both the arrays of substructures and the SMILES prefix, by passing them through embedding layers (Figure 2). The purpose of the embedding layers is to encode the tokens of the input data (e.g., words in a sentence or SMILES characters) into a numerical vector representation more amenable for fitting our models to (e.g., word2vec). The embedding layers themselves are learnable.\n",
    "\n",
    "As part of the embedding layer, we will also apply positional encoding. In doing so, we effectively apply a position dependent function to the input arrays. This has the purpose of injecting some information about the ordering of tokens in our input data. Our model will make use of sinusoidal positional encodings of the following form\n",
    "\n",
    "$$PE_{pos,i} = sin(pos/10000^{2i/d_{model}})$$\n",
    "$$PE_{pos,2i+1} = cos(pos/10000^{2i/d_{model}})$$\n",
    "\n",
    "In the code block below, complete the implementation of the PositionaEncoding class. You can verify whether you have correctly implemented things by checking if the example gives you something like the following contour plot (for an input with dimension 957 and an output with dimension $d_{model}=128$):\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"figures/positionalencoding.png\" width=\"30%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed71ae-11e4-4aba-b35f-93b66d862469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    \"\"\" Positional encoding with option of selecting specific indices to add \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 30000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        ### MODIFY BELOW ###\n",
    "\n",
    "        ### MODIFY ABOVE ###\n",
    "    \n",
    "    def forward(self, x: Tensor, ind: Tensor) -> Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "            ind: Tensor, shape [batch_size, seq_len] or NoneType\n",
    "        '''\n",
    "        if ind is not None:\n",
    "            added_pe = self.pe[torch.arange(1).reshape(-1, 1), ind, :]\n",
    "            x = x + added_pe\n",
    "        else:\n",
    "            ### MODIFY BELOW ###\n",
    "\n",
    "            ### MODIFY ABOVE ###\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "posencode = PositionalEncoding(128, dropout=0)\n",
    "x = torch.ones((1,957,128))\n",
    "y = posencode(x, None)\n",
    "plt.contourf(y.detach().numpy()[0].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec6901-4628-4f90-b767-7d54dde608e2",
   "metadata": {},
   "source": [
    "With the positional encoder implemented, let us now combine it with embedding layers to complete the first input processing steps for our model. Implement the functions below, which will specify the embedding and encoding of our source (i.e., substructure inputs) and targets (i.e., SMILES string prefix). The two embedding/encoding forward procedures should effectively be the same but we will separate them to keep things fully general.\n",
    "\n",
    "I would suggest just using the [nn.Embedding](https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html) Pytorch module here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57cfd1c-8be8-42fd-95f4-c2632b269f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_fwd_fxn_basic(src: Tensor,\n",
    "                      d_model: int,\n",
    "                      src_embed: nn.Module = nn.Embedding,\n",
    "                      src_pad_token: int = 0,\n",
    "                      pos_encoder: nn.Module = nn.Embedding) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    \"\"\"Forward processing for source tensor in Transformer for substructure to structure problem\n",
    "    Args:\n",
    "        src: The unembedded source tensor, raw input into the forward() method, \n",
    "            shape (batch_size, seq_len)\n",
    "        d_model: The dimensionality of the model\n",
    "        src_embed: The source embedding layer\n",
    "        src_pad_token: The source padding token index\n",
    "        pos_encoder: The positional encoder layer\n",
    "    \"\"\"\n",
    "    if not isinstance(src_embed, nn.Embedding):\n",
    "        src_key_pad_mask = None\n",
    "    elif src_pad_token is not None:\n",
    "        src_key_pad_mask = (src == src_pad_token).bool().to(src.device)\n",
    "\n",
    "    ### MODIFY BELOW ###\n",
    "\n",
    "    \n",
    "    ### MODIFY ABOVE ###\n",
    "    return src, src_key_pad_mask\n",
    "\n",
    "\n",
    "def tgt_fwd_fxn_basic(tgt: Tensor,\n",
    "                      d_model: int, \n",
    "                      tgt_embed: nn.Module = nn.Embedding,\n",
    "                      tgt_pad_token: int = 21,\n",
    "                      pos_encoder: nn.Module = nn.Embedding) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    \"\"\"Standard forward processing function for target tensor used in the Transformer network \n",
    "    Args:\n",
    "        tgt: The unembedded target tensor, raw input into the forward() method,\n",
    "            shape (batch_size, seq_len)\n",
    "        d_model: The dimensionality of the model\n",
    "        tgt_embed: The target embedding layer\n",
    "        tgt_pad_token: The target padding token index\n",
    "        pos_encoder: The positional encoder layer\n",
    "    \"\"\"\n",
    "    if tgt_pad_token is not None:\n",
    "        tgt_key_pad_mask = (tgt == tgt_pad_token).bool().to(tgt.device)\n",
    "    else:\n",
    "        tgt_key_pad_mask = None\n",
    "\n",
    "    ### MODIFY BELOW ###\n",
    "\n",
    "    ### MODIFY ABOVE ###\n",
    "    return tgt, tgt_key_pad_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751ad31-dc17-4bed-baf9-7b1c9e1aef33",
   "metadata": {},
   "source": [
    "Now let us construct our Transformer model. We will be employing a layer of abstraction to match the design of the model implementation in NMR2Struct, but you will only be asked here to complete the implementation of the Transformer class (not the abstract TransformerModel class). In particular, to complete the Transformer class you will need to initialize it properly and also implement the forward routine. Here I would suggest you make use of the [nn.Transformer](https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html) from Pytorch as well as the embedding routines you already completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1250836-9a41-466a-86b9-bada08f968de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    model_id = \"Transformer\"\n",
    "    \n",
    "    def __init__(self, src_embed: nn.Module, tgt_embed: nn.Module,\n",
    "                 src_pad_token: int, tgt_pad_token: int, \n",
    "                 src_forward_function: Callable[[Tensor, nn.Module, int, Optional[nn.Module]], Tuple[Tensor, Optional[Tensor]]],\n",
    "                 tgt_forward_function: Callable[[Tensor, nn.Module, int, Optional[nn.Module]], Tuple[Tensor, Optional[Tensor]]],\n",
    "                 d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6, num_decoder_layers: int = 6,\n",
    "                 dim_feedforward: int = 2048, dropout: float = 0.1, activation: str = 'relu', custom_encoder: Optional[Any] = None,\n",
    "                 custom_decoder: Optional[Any] = None, target_size: int = 50, source_size: int = 957,\n",
    "                 layer_norm_eps: float = 1e-05, batch_first: bool = True, norm_first: bool = False, \n",
    "                 device: torch.device = None, dtype: torch.dtype = torch.float):\n",
    "        \n",
    "        r\"\"\"Most parameters are standard for the PyTorch transformer class. A few specific ones that have been added:\n",
    "\n",
    "        src_embed: The embedding module for the src tensor passed to the model\n",
    "        tgt_embed: The embedding module for the tgt tensor passed to the model\n",
    "        src_pad_token: The index used to indicate padding in the source sequence\n",
    "        tgt_pad_token: The index used to indicate padding in the target sequence\n",
    "        src_forward_function: A function that processes the src tensor using the src embedding, src pad token, and positional encoding to generate\n",
    "            the embedded src and the src_key_pad_mask\n",
    "        tgt_forward_function: A function that processes the tgt tensor using the tgt embedding, tgt pad token, and positional encoding to generate\n",
    "            the embedded tgt and the tgt_key_pad_mask\n",
    "        target_size (int): Size of the target alphabet (including start, stop, and pad tokens).\n",
    "        source_size (int): Size of the source alphabet (including start, stop, and pad tokens).\n",
    "        batch_first is set to be default True, more intuitive to reason about dimensionality if batching \n",
    "            dimension is first\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_fwd_fn = src_forward_function\n",
    "        self.tgt_fwd_fn = tgt_forward_function\n",
    "        self.src_pad_token = src_pad_token\n",
    "        self.tgt_pad_token = tgt_pad_token\n",
    "\n",
    "        self.tgt_size = target_size\n",
    "        self.src_size = source_size\n",
    "        self.d_model = d_model\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        ### MODIFY BELOW ###\n",
    "\n",
    "        ### MODIFY ABOVE ###\n",
    "\n",
    "    def _get_tgt_mask(self, size):\n",
    "        #Generate a mask for the target to preserve autoregressive property. Note that the mask is \n",
    "        #   Additive for the PyTorch transformer\n",
    "        mask = torch.tril(torch.ones(size, size, dtype = self.dtype, device = self.device))\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf'))\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def _sanitize_forward_args(self, \n",
    "                               x: Tuple[Tensor, Tuple],\n",
    "                               y: Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        '''Processes raw x, y inputs to the transformer for use in forward()\n",
    "        Args:\n",
    "            x: Tuple of a tensor (input) and the set of smiles strings (smiles)\n",
    "            y: A tuple of a the shifted target tensor and full target tensor\n",
    "        '''\n",
    "        #Unpack the tuples\n",
    "        inp, _ = x\n",
    "        shifted_y, _ = y\n",
    "        if isinstance(self.src_embed, nn.Embedding):\n",
    "            inp = inp.long()\n",
    "        if isinstance(self.tgt_embed, nn.Embedding):\n",
    "            shifted_y = shifted_y.long()\n",
    "        return inp, shifted_y\n",
    "    \n",
    "    #Sketch of what the forward function could look like with more abstraction\n",
    "    def forward(self, \n",
    "                x: Tuple[Tensor, Tuple], \n",
    "                y: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "        src, tgt = self._sanitize_forward_args(x, y)\n",
    "        tgt_mask = self._get_tgt_mask(tgt.size(1)).to(tgt.device)\n",
    "        ### MODIFY BELOW ###\n",
    "\n",
    "        ### MODIFY ABOVE ###\n",
    "        return out\n",
    "\n",
    "    def get_loss(self,\n",
    "                 x: Tuple[Tensor, Tuple], \n",
    "                 y: Tuple[Tensor], \n",
    "                 loss_fn: Callable[[Tensor, Tensor], Tensor]) -> Tensor:\n",
    "        \"\"\"\n",
    "        Unpacks the input and obtains the loss value\n",
    "        Args:\n",
    "            x: Tuple of a tensor (input) and the set of smiles strings (smiles)\n",
    "            y: A tuple of a the shifted target tensor and full target tensor\n",
    "            loss_fn: The loss function to use for the model, with the signature\n",
    "                tensor, tensor -> tensor\n",
    "        \"\"\"\n",
    "        pred = self.forward(x, y)\n",
    "        pred = pred.permute(0, 2, 1)\n",
    "        _, full_y = y\n",
    "        if isinstance(self.tgt_embed, nn.Embedding):\n",
    "            full_y = full_y.long()\n",
    "        loss = loss_fn(pred, full_y.to(self.device))\n",
    "        return loss\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\" Example model wrapper for transformer network \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 src_embed: str, \n",
    "                 src_embed_options: dict,\n",
    "                 tgt_embed: str, \n",
    "                 tgt_embed_options: dict,\n",
    "                 src_pad_token: int, \n",
    "                 tgt_pad_token: int,\n",
    "                 src_forward_function: str, \n",
    "                 tgt_forward_function: str, \n",
    "                 freeze_components: Optional[list] = None,\n",
    "                 d_model: int = 512, \n",
    "                 nhead: int = 8, \n",
    "                 num_encoder_layers: int = 6, \n",
    "                 num_decoder_layers: int = 6,\n",
    "                 dim_feedforward: int = 2048, \n",
    "                 dropout: float = 0.1, \n",
    "                 activation: str = 'relu', \n",
    "                 custom_encoder: Optional[Any] = None,\n",
    "                 custom_decoder: Optional[Any] = None, \n",
    "                 target_size: int = 50, \n",
    "                 source_size: int = 957,\n",
    "                 layer_norm_eps: float = 1e-05, \n",
    "                 batch_first: bool = True, \n",
    "                 norm_first: bool = False, \n",
    "                 device: torch.device = None, \n",
    "                 dtype: torch.dtype = torch.float):\n",
    "        r\"\"\"Most parameters are standard for the PyTorch transformer class. A few specific ones that have been added:\n",
    "\n",
    "        src_embed: The name of the embedding module for the src tensor passed to the model\n",
    "        src_embed_options: Dictionary of options for the src embedding module\n",
    "        tgt_embed: The name of the embedding module for the tgt tensor passed to the model\n",
    "        tgt_embed_options: Dictionary of options for the tgt embedding module\n",
    "        src_pad_token: The index used to indicate padding in the source sequence\n",
    "        tgt_pad_token: The index used to indicate padding in the target sequence\n",
    "        src_forward_function: Name of the function that processes the src tensor using the src embedding, src pad token, and positional encoding to generate\n",
    "            the embedded src and the src_key_pad_mask\n",
    "        tgt_forward_function: Name of the function that processes the tgt tensor using the tgt embedding, tgt pad token, and positional encoding to generate\n",
    "            the embedded tgt and the tgt_key_pad_mask\n",
    "        freeze_components: List of component names to freeze\n",
    "        target_size (int): Size of the target alphabet (including start, stop, and pad tokens).\n",
    "        source_size (int): Size of the source alphabet (including start, stop, and pad tokens).\n",
    "        batch_first is set to be default True, more intuitive to reason about dimensionality if batching \n",
    "            dimension is first\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        ### MODIFY BELOW ###\n",
    "\n",
    "        ### MODIFY ABOVE ###\n",
    "        \n",
    "        self.network = Transformer( src_embed_layer, tgt_embed_layer,\n",
    "                                    src_pad_token, tgt_pad_token, \n",
    "                                    src_forward_function, tgt_forward_function,\n",
    "                                    d_model, nhead, num_encoder_layers, num_decoder_layers,\n",
    "                                    dim_feedforward, dropout, activation, custom_encoder,\n",
    "                                    custom_decoder, target_size, source_size,\n",
    "                                    layer_norm_eps, batch_first, norm_first, device, dtype)\n",
    "        self.initialize_weights()\n",
    "        self.freeze_components = freeze_components\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def initialize_weights(self) -> None:\n",
    "        \"\"\"Initializes network weights\n",
    "        Non-1D parameters are initialized using Xavier initialization\n",
    "        \"\"\"\n",
    "        for p in self.network.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def freeze(self) -> None:\n",
    "        \"\"\"Disables gradients for specific components of the network\n",
    "        \n",
    "        Args:\n",
    "            components: A list of strings corresponding to the model components\n",
    "                to freeze, e.g. src_embed, tgt_embed.\n",
    "        \"\"\"\n",
    "        #TODO: This will need careful testing\n",
    "        if self.freeze_components is not None:\n",
    "            for component in self.freeze_components:\n",
    "                if hasattr(self.network, component):\n",
    "                    for param in getattr(self.network, component).parameters():\n",
    "                        param.requires_grad = False\n",
    "    \n",
    "    def forward(self, \n",
    "                x: Tuple[Tensor, Tuple], \n",
    "                y: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "        return self.network(x, y)\n",
    "    \n",
    "    def get_loss(self,\n",
    "                 x: Tuple[Tensor, Tuple], \n",
    "                 y: Tuple[Tensor], \n",
    "                 loss_fn: Callable[[Tensor, Tensor], Tensor]) -> Tensor:\n",
    "        return self.network.get_loss(x, y, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064efa3c-077d-408b-8f4b-c95d430b243f",
   "metadata": {},
   "source": [
    "In the following sections we will load the substructure to structure model you have been training and attempt to use it for structure prediction. The subsequent sections will only run if you have implemented the model correctly and so will serve as a check for your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f4804-fe65-4864-b810-749fc8c86317",
   "metadata": {},
   "source": [
    "## Loading our substructure to structure model\n",
    "\n",
    "While you were implementing your model, hopefully your model has also been training. If it has trained to a point where the loss over the validation set has seemingly plateaud, copy the contents of the \"checkpoints\" subfolder into a folder in the directory here named \"model1\". If training has not seemingly saturated, you can instead use the pretrained model provided (see \"transformer_checkpoint.ckpt\" provided by the NMR2Struct package [here](https://github.com/MarklandGroup/NMR2Struct/tree/main/checkpoints)).\n",
    "\n",
    "Below we will try to load the trained model using just the code in this notebook, which is a stripped down reimplementation of the relevant code from the NMR2Struct package that we used to train this model. If you receive an error here, you will likely need to debug your implementation of the model in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a04a77-ea28-4ad9-887f-8a011f511158",
   "metadata": {},
   "outputs": [],
   "source": [
    "listdoc =  yaml.safe_load(open('model1/full_inference_config.yaml', 'r'))\n",
    "model_args = listdoc['model']\n",
    "model_config = model_args['model_args']\n",
    "\n",
    "model = TransformerModel(dtype=torch.float32, device=torch.device('cpu'), **model_config)\n",
    "ckpt = torch.load('model1/model_epoch=397_loss=0.07503651.pt', map_location=torch.device('cpu'))['model_state_dict']\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f92bae-96a6-4eed-9cbe-86899991c0a2",
   "metadata": {},
   "source": [
    "## Inference with the substructure to structure model\n",
    "\n",
    "With the model loaded, let us now attempt to use it to predict molecular structure given the constituent substructures. For our model, instead of passing it a binary encoding of the substructures (as is specified by the substructures.h5 dataset) we instead pass a zero-padded array that lists in order of index which substructures are present. We 1-index these listed substructures because we reserve zero for padding. Complete the function below to apply this conversion to the substructure input data. If implemented correctly, running the code block below should output the following array for the example provided:\n",
    "\n",
    "```\n",
    "array([104, 167, 171, 180, 183, 184, 211, 212, 217, 225, 231, 278, 594,\n",
    "       646, 647, 649, 651, 652, 653, 654, 656, 717, 720, 721, 749, 765,\n",
    "       869, 870, 879, 880, 887, 908, 912, 924, 949, 955, 957,   0,   0,\n",
    "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "         0,   0,   0,   0,   0,   0,   0,   0,   0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bad1cd-3793-4d77-81b0-7434f5e818b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substruct_transform(substructures, max_len, pad_token=0):\n",
    "    \"\"\"Transforms the input binary substructure array into shifted and padded 1-indexed array\"\"\"\n",
    "    ### MODIFY BELOW ###\n",
    "\n",
    "    ### MODIFY ABOVE ###\n",
    "\n",
    "\n",
    "i = 10000\n",
    "hf = h5py.File('data/substructures_test.h5', 'r')\n",
    "isubs = hf['substructure_labels'][i]\n",
    "hf.close()\n",
    "\n",
    "max_len = 74\n",
    "substruct_transform(isubs, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90fe13-143d-41b9-bd85-b7d2c42ea81a",
   "metadata": {},
   "source": [
    "Now let us run the code block below to use our Transformer model to make a prediction for the first character of the molecule's SMILES string given the list of substructures for that molecule. Which character does our model predict to most likely be the first character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a4be71-33d4-46ff-9bc7-6efc1d207463",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = np.load('model1/alphabet.npy')\n",
    "start_token = 22\n",
    "stop_token = 23\n",
    "\n",
    "working_x = torch.tensor(substruct_transform(isubs, max_len))[None,:]\n",
    "working_y = torch.tensor([start_token])[None,:]\n",
    "working_token_probs = torch.tensor([])\n",
    "\n",
    "x = (working_x, None)\n",
    "y = (working_y, None)\n",
    "next_pos = model(x, y)\n",
    "next_val = next_pos[:, -1, :]\n",
    "char_probs = torch.nn.functional.softmax(next_val, dim = -1)\n",
    "char_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae0fc2b-a6b9-45b9-a0f8-505d1e8dad9a",
   "metadata": {},
   "source": [
    "Now let us run prediction with our model again but now with the SMILES prefix that we now have after predicting for the first character in the previous code block to obtain the second predicted character of the SMILES string. Note that the number 22 serves as our SMILES string start character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba570a85-68cf-4e33-92da-1b32739ddef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_sample_batched(k_val: int | float , \n",
    "                             character_probabilities: Tensor) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Generates the next character using top-k sampling scheme.\n",
    "\n",
    "    In top-k sampling, the probability mass is redistributed among the\n",
    "    top-k next tokens, where k is a hyperparameter. Once redistributed, \n",
    "    the next token is sampled from the top-k tokens.\n",
    "    \"\"\"\n",
    "    top_values, top_indices = torch.topk(character_probabilities, k_val, sorted = True)\n",
    "    #Take the sum of the top probabilities and renormalize\n",
    "    tot_probs = top_values / torch.sum(top_values, dim = -1).reshape(-1, 1)\n",
    "    #Sample from the top k probabilities. This represents a multinomial distribution\n",
    "    try:\n",
    "        assert(torch.allclose(torch.sum(tot_probs, dim = -1), torch.tensor(1.0)))\n",
    "    except:\n",
    "        print(\"Probabilities did not pass allclose check!\")\n",
    "        print(f\"Sum of probs is {torch.sum(tot_probs)}\")\n",
    "    selected_index = torch.multinomial(tot_probs, 1)\n",
    "    #For gather to work, both tensors have to have the same number of dimensions:\n",
    "    if len(top_indices.shape) != len(selected_index.shape):\n",
    "        top_indices = top_indices.reshape(selected_index.shape[0], -1)\n",
    "    output = torch.gather(top_indices, -1, selected_index)\n",
    "    output_token_probs = torch.gather(tot_probs, -1, selected_index)\n",
    "    return output, output_token_probs\n",
    "\n",
    "\n",
    "sample_val = 5\n",
    "selected_indices, token_probs = get_top_k_sample_batched(sample_val, char_probs)\n",
    "\n",
    "working_y = torch.cat((working_y, selected_indices), dim = -1)\n",
    "working_token_probs = torch.cat((working_token_probs, token_probs), dim = -1)\n",
    "\n",
    "print(working_y)\n",
    "print(alphabet[working_y[0,1:]])\n",
    "\n",
    "x = (working_x, None)\n",
    "y = (working_y, None)\n",
    "next_pos = model(x, y)\n",
    "next_val = next_pos[:, -1, :]\n",
    "char_probs = torch.nn.functional.softmax(next_val, dim = -1)\n",
    "\n",
    "selected_indices, token_probs = get_top_k_sample_batched(sample_val, char_probs)\n",
    "\n",
    "working_y = torch.cat((working_y, selected_indices), dim = -1)\n",
    "working_token_probs = torch.cat((working_token_probs, token_probs), dim = -1)\n",
    "\n",
    "print(working_y)\n",
    "print(alphabet[working_y[0,1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2932783d-4438-4432-8d79-4d73b8df3d4d",
   "metadata": {},
   "source": [
    "Write a routine that now makes autoregressive character predictions until we reach a stop token, which in our case will be the number 23, and thus have a completed molecule predicted. Have the routine return both the SMILES string of the generated structure as well as an array of all the token probabilities. Is your predicted molecule the correct one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e74576-0091-469f-93f8-7e22d3104271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_structure(model, substruct_list, start_token=22, stop_token=23, max_len=74, sample_val=5, max_steps=100):\n",
    "    ### MODIFY BELOW ###\n",
    "\n",
    "    ### MODIFY ABOVE ###\n",
    "\n",
    "pred_tokens, pred_token_probs = generate_structure(model, isubs)\n",
    "pred_smi = ''\n",
    "for ichar in alphabet[pred_tokens[0,1:-1]]:\n",
    "    pred_smi+=ichar\n",
    "print('Target: ' + ismi)\n",
    "print('Predicted: ' + pred_smi)\n",
    "print('Predicted Score: ' + str(np.log(pred_token_probs[0,:-1].detach().numpy()).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40258e19-526c-4fad-a55c-5388c57d0e79",
   "metadata": {},
   "source": [
    "You may or may not have predicted for the correct molecule in running the previous code block the first time. Given that the predictions made by this model are probabilistic, if you run that code block again you might sample a different final molecular structure. In practice we will want to make a set of predictions for a given input and then rank them by which our model believes is most likely the actual molecular structure. Write a routine that accomplishes this below, predicting 10 molecules and saving for each molecule its SMILES string and its log probability\n",
    "\n",
    "Be sure to exclude molecules that are to chemically feasible (can you use rdKit for this? Hint: try visualizing all your generated structures using the subsequent code block) and duplicate structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ea800-7f69-4382-bbe5-e10f4dfb0864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_k_structures(model, substruct_list, num_pred_per_tgt):\n",
    "    ### MODIFY BELOW ###\n",
    "\n",
    "    ### MODIFY ABOVE ###\n",
    "\n",
    "\n",
    "sampled_smis, sampled_smi_scores = generate_k_structures(model, isubs, 10)\n",
    "sampled_smi_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df7cb2-d674-4a13-bcca-ce8571895880",
   "metadata": {},
   "source": [
    "Use the below code block to visualize the 10 predicted molecules as ordered by their log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d200fa-317c-4dcb-bc5e-cc8ec0de7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_mols = [Chem.MolFromSmiles(x) for x in sampled_smis]\n",
    "Chem.Draw.MolsToGridImage(sampled_mols,molsPerRow=5,subImgSize=(200,200), legends=[str(x) for x in sampled_smis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d65a06-5041-45f9-a076-31ac7bdd5ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
